{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CMAPS data pre-process understanding \n",
    "\n",
    "# %pip install torch\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "# from numpy.core.fromnumeric import transpose\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy import interpolate\n",
    "import math\n",
    "import torch\n",
    "\n",
    "import torch.utils.data as data\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeing the stucture of the orginal data\n",
      "Training Data Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 61249 entries, 0 to 61248\n",
      "Data columns (total 26 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        61249 non-null  int64  \n",
      " 1   cycle     61249 non-null  int64  \n",
      " 2   setting1  61249 non-null  float64\n",
      " 3   setting2  61249 non-null  float64\n",
      " 4   setting3  61249 non-null  float64\n",
      " 5   s1        61249 non-null  float64\n",
      " 6   s2        61249 non-null  float64\n",
      " 7   s3        61249 non-null  float64\n",
      " 8   s4        61249 non-null  float64\n",
      " 9   s5        61249 non-null  float64\n",
      " 10  s6        61249 non-null  float64\n",
      " 11  s7        61249 non-null  float64\n",
      " 12  s8        61249 non-null  float64\n",
      " 13  s9        61249 non-null  float64\n",
      " 14  s10       61249 non-null  float64\n",
      " 15  s11       61249 non-null  float64\n",
      " 16  s12       61249 non-null  float64\n",
      " 17  s13       61249 non-null  float64\n",
      " 18  s14       61249 non-null  float64\n",
      " 19  s15       61249 non-null  float64\n",
      " 20  s16       61249 non-null  float64\n",
      " 21  s17       61249 non-null  int64  \n",
      " 22  s18       61249 non-null  int64  \n",
      " 23  s19       61249 non-null  float64\n",
      " 24  s20       61249 non-null  float64\n",
      " 25  s21       61249 non-null  float64\n",
      "dtypes: float64(22), int64(4)\n",
      "memory usage: 12.6 MB\n",
      "None\n",
      "Number of rows/ time steps: 61249\n",
      "Number of columns: 26\n",
      "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
      "0   1      1   42.0049    0.8400     100.0  445.00  549.68  1343.43  1112.93   \n",
      "1   1      2   20.0020    0.7002     100.0  491.19  606.07  1477.61  1237.50   \n",
      "2   1      3   42.0038    0.8409     100.0  445.00  548.95  1343.12  1117.05   \n",
      "3   1      4   42.0000    0.8400     100.0  445.00  548.70  1341.24  1118.03   \n",
      "4   1      5   25.0063    0.6207      60.0  462.54  536.10  1255.23  1033.59   \n",
      "5   1      6   34.9996    0.8400     100.0  449.44  554.77  1352.87  1117.01   \n",
      "\n",
      "     s5  ...     s12      s13      s14      s15   s16  s17   s18     s19  \\\n",
      "0  3.91  ...  129.78  2387.99  8074.83   9.3335  0.02  330  2212  100.00   \n",
      "1  9.35  ...  312.59  2387.73  8046.13   9.1913  0.02  361  2324  100.00   \n",
      "2  3.91  ...  129.62  2387.97  8066.62   9.4007  0.02  329  2212  100.00   \n",
      "3  3.91  ...  129.80  2388.02  8076.05   9.3369  0.02  328  2212  100.00   \n",
      "4  7.05  ...  164.11  2028.08  7865.80  10.8366  0.02  305  1915   84.93   \n",
      "5  5.48  ...  181.90  2387.87  8054.10   9.3346  0.02  330  2223  100.00   \n",
      "\n",
      "     s20      s21  \n",
      "0  10.62   6.3670  \n",
      "1  24.37  14.6552  \n",
      "2  10.48   6.4213  \n",
      "3  10.54   6.4176  \n",
      "4  14.03   8.6754  \n",
      "5  14.91   8.9057  \n",
      "\n",
      "[6 rows x 26 columns]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Data Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41214 entries, 0 to 41213\n",
      "Data columns (total 26 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        41214 non-null  int64  \n",
      " 1   cycle     41214 non-null  int64  \n",
      " 2   setting1  41214 non-null  float64\n",
      " 3   setting2  41214 non-null  float64\n",
      " 4   setting3  41214 non-null  float64\n",
      " 5   s1        41214 non-null  float64\n",
      " 6   s2        41214 non-null  float64\n",
      " 7   s3        41214 non-null  float64\n",
      " 8   s4        41214 non-null  float64\n",
      " 9   s5        41214 non-null  float64\n",
      " 10  s6        41214 non-null  float64\n",
      " 11  s7        41214 non-null  float64\n",
      " 12  s8        41214 non-null  float64\n",
      " 13  s9        41214 non-null  float64\n",
      " 14  s10       41214 non-null  float64\n",
      " 15  s11       41214 non-null  float64\n",
      " 16  s12       41214 non-null  float64\n",
      " 17  s13       41214 non-null  float64\n",
      " 18  s14       41214 non-null  float64\n",
      " 19  s15       41214 non-null  float64\n",
      " 20  s16       41214 non-null  float64\n",
      " 21  s17       41214 non-null  int64  \n",
      " 22  s18       41214 non-null  int64  \n",
      " 23  s19       41214 non-null  float64\n",
      " 24  s20       41214 non-null  float64\n",
      " 25  s21       41214 non-null  float64\n",
      "dtypes: float64(22), int64(4)\n",
      "memory usage: 8.5 MB\n",
      "None\n",
      "Number of rows/ time steps: 41214\n",
      "Number of columns: 26\n",
      "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
      "0   1      1   20.0072    0.7000     100.0  491.19  606.67  1481.04  1227.81   \n",
      "1   1      2   24.9984    0.6200      60.0  462.54  536.22  1256.17  1031.48   \n",
      "2   1      3   42.0000    0.8420     100.0  445.00  549.23  1340.13  1105.88   \n",
      "3   1      4   42.0035    0.8402     100.0  445.00  549.19  1339.70  1107.26   \n",
      "4   1      5   35.0079    0.8400     100.0  449.44  555.10  1353.04  1117.80   \n",
      "5   1      6   25.0010    0.6203      60.0  462.54  536.40  1255.38  1043.74   \n",
      "\n",
      "     s5  ...     s12      s13      s14      s15   s16  s17   s18     s19  \\\n",
      "0  9.35  ...  313.03  2387.78  8048.98   9.2229  0.02  362  2324  100.00   \n",
      "1  7.05  ...  163.61  2028.09  7863.46  10.8632  0.02  306  1915   84.93   \n",
      "2  3.91  ...  129.98  2387.95  8071.13   9.3960  0.02  328  2212  100.00   \n",
      "3  3.91  ...  129.48  2387.90  8078.89   9.3594  0.02  328  2212  100.00   \n",
      "4  5.48  ...  181.82  2387.87  8057.83   9.3030  0.02  333  2223  100.00   \n",
      "5  7.05  ...  163.95  2028.12  7867.38  10.8661  0.02  304  1915   84.93   \n",
      "\n",
      "     s20      s21  \n",
      "0  24.31  14.7007  \n",
      "1  14.36   8.5748  \n",
      "2  10.39   6.4365  \n",
      "3  10.56   6.2367  \n",
      "4  14.85   8.9326  \n",
      "5  14.17   8.6135  \n",
      "\n",
      "[6 rows x 26 columns]\n",
      "\n",
      "\n",
      "\n",
      "Test Truth Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       248 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 2.1 KB\n",
      "None\n",
      "     0\n",
      "0   22\n",
      "1   39\n",
      "2  107\n",
      "3   75\n",
      "4  149\n",
      "5   78\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We can see how magnitude of difference sensor data can vary -> thats why we need to norlamize the data later\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now the processing data function\n",
      "train_rul visualization\n",
      "\n",
      "As we found already for each id the max cycle\n",
      "   id  max\n",
      "0   1  321\n",
      "1   2  299\n",
      "2   3  307\n",
      "3   4  274\n",
      "4   5  193\n",
      "5   6  331\n",
      "6   7  221\n",
      "\n",
      "After merging with the max cycle for corresponding id\n",
      "   id  cycle  setting1  setting2  setting3      s1      s2       s3       s4  \\\n",
      "0   1      1   42.0049    0.8400     100.0  445.00  549.68  1343.43  1112.93   \n",
      "1   1      2   20.0020    0.7002     100.0  491.19  606.07  1477.61  1237.50   \n",
      "2   1      3   42.0038    0.8409     100.0  445.00  548.95  1343.12  1117.05   \n",
      "3   1      4   42.0000    0.8400     100.0  445.00  548.70  1341.24  1118.03   \n",
      "4   1      5   25.0063    0.6207      60.0  462.54  536.10  1255.23  1033.59   \n",
      "\n",
      "     s5  ...      s13      s14      s15   s16  s17   s18     s19    s20  \\\n",
      "0  3.91  ...  2387.99  8074.83   9.3335  0.02  330  2212  100.00  10.62   \n",
      "1  9.35  ...  2387.73  8046.13   9.1913  0.02  361  2324  100.00  24.37   \n",
      "2  3.91  ...  2387.97  8066.62   9.4007  0.02  329  2212  100.00  10.48   \n",
      "3  3.91  ...  2388.02  8076.05   9.3369  0.02  328  2212  100.00  10.54   \n",
      "4  7.05  ...  2028.08  7865.80  10.8366  0.02  305  1915   84.93  14.03   \n",
      "\n",
      "       s21  max  \n",
      "0   6.3670  321  \n",
      "1  14.6552  321  \n",
      "2   6.4213  321  \n",
      "3   6.4176  321  \n",
      "4   8.6754  321  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "We now also have access to the actual RUL that it will decay for each machine, at their own timestep\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "we remove sensors with constant values , specifically those with sensor indices 1,5,6,10,16,18,and 19.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now used the max to cal the RUL at each time step\n",
      "max is then removed from the data\n",
      "We can now observe that some sensor data are removed\n",
      "   id  cycle  setting1  setting2  setting3      s2       s3       s4      s7  \\\n",
      "0   1      1   42.0049    0.8400     100.0  549.68  1343.43  1112.93  137.36   \n",
      "1   1      2   20.0020    0.7002     100.0  606.07  1477.61  1237.50  332.10   \n",
      "2   1      3   42.0038    0.8409     100.0  548.95  1343.12  1117.05  138.18   \n",
      "3   1      4   42.0000    0.8400     100.0  548.70  1341.24  1118.03  137.98   \n",
      "4   1      5   25.0063    0.6207      60.0  536.10  1255.23  1033.59  174.82   \n",
      "5   1      6   34.9996    0.8400     100.0  554.77  1352.87  1117.01  193.82   \n",
      "6   1      7    0.0019    0.0001     100.0  641.83  1583.47  1393.89  552.45   \n",
      "7   1      8   41.9981    0.8400     100.0  549.05  1344.16  1110.77  137.13   \n",
      "8   1      9   42.0016    0.8400     100.0  549.55  1342.85  1101.67  138.02   \n",
      "9   1     10   25.0019    0.6217      60.0  536.35  1251.91  1041.37  174.70   \n",
      "\n",
      "        s8       s9    s11     s12      s13      s14      s15  s17    s20  \\\n",
      "0  2211.86  8311.32  41.69  129.78  2387.99  8074.83   9.3335  330  10.62   \n",
      "1  2323.66  8713.60  43.94  312.59  2387.73  8046.13   9.1913  361  24.37   \n",
      "2  2211.92  8306.69  41.66  129.62  2387.97  8066.62   9.4007  329  10.48   \n",
      "3  2211.88  8312.35  41.68  129.80  2388.02  8076.05   9.3369  328  10.54   \n",
      "4  1915.22  7994.94  36.48  164.11  2028.08  7865.80  10.8366  305  14.03   \n",
      "5  2222.77  8340.00  41.44  181.90  2387.87  8054.10   9.3346  330  14.91   \n",
      "6  2387.92  9050.50  46.94  520.48  2387.89  8127.92   8.3960  391  38.93   \n",
      "7  2211.92  8307.28  41.60  129.65  2387.97  8075.99   9.3679  329  10.55   \n",
      "8  2211.90  8307.81  41.44  129.65  2388.00  8071.13   9.3384  328  10.63   \n",
      "9  1915.23  8005.83  36.24  164.08  2028.13  7869.41  10.9141  305  14.34   \n",
      "\n",
      "       s21  \n",
      "0   6.3670  \n",
      "1  14.6552  \n",
      "2   6.4213  \n",
      "3   6.4176  \n",
      "4   8.6754  \n",
      "5   8.9057  \n",
      "6  23.4578  \n",
      "7   6.2787  \n",
      "8   6.3055  \n",
      "9   8.6119  \n",
      "Normalising of training data\n",
      "Get the max cycle for each engine and calculate the RUL at each time step, also cappingt the max rul at self defined maximal RUL\n",
      "Using max - current cycle = RUL remaining at each time step\n",
      "Therefore getting our y labels for training data\n",
      "   id  RUL\n",
      "0   1  125\n",
      "1   1  125\n",
      "2   1  125\n",
      "3   1  125\n",
      "4   1  125\n",
      "5   1  125\n",
      "6   1  125\n",
      "7   1  125\n",
      "8   1  125\n",
      "9   1  125\n",
      "we can see that all RUL is 125, this is because it's RUL is capped at 125, the acutal RUL is actually more than 125\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Now processing the test data\n",
      "Test data summary before processing\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41214 entries, 0 to 41213\n",
      "Data columns (total 26 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        41214 non-null  int64  \n",
      " 1   cycle     41214 non-null  int64  \n",
      " 2   setting1  41214 non-null  float64\n",
      " 3   setting2  41214 non-null  float64\n",
      " 4   setting3  41214 non-null  float64\n",
      " 5   s1        41214 non-null  float64\n",
      " 6   s2        41214 non-null  float64\n",
      " 7   s3        41214 non-null  float64\n",
      " 8   s4        41214 non-null  float64\n",
      " 9   s5        41214 non-null  float64\n",
      " 10  s6        41214 non-null  float64\n",
      " 11  s7        41214 non-null  float64\n",
      " 12  s8        41214 non-null  float64\n",
      " 13  s9        41214 non-null  float64\n",
      " 14  s10       41214 non-null  float64\n",
      " 15  s11       41214 non-null  float64\n",
      " 16  s12       41214 non-null  float64\n",
      " 17  s13       41214 non-null  float64\n",
      " 18  s14       41214 non-null  float64\n",
      " 19  s15       41214 non-null  float64\n",
      " 20  s16       41214 non-null  float64\n",
      " 21  s17       41214 non-null  int64  \n",
      " 22  s18       41214 non-null  int64  \n",
      " 23  s19       41214 non-null  float64\n",
      " 24  s20       41214 non-null  float64\n",
      " 25  s21       41214 non-null  float64\n",
      "dtypes: float64(22), int64(4)\n",
      "memory usage: 8.5 MB\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "test truth summary before processing\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       248 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 2.1 KB\n",
      "None\n",
      "     0\n",
      "0   22\n",
      "1   39\n",
      "2  107\n",
      "3   75\n",
      "4  149\n",
      "5   78\n",
      "6   94\n",
      "7   14\n",
      "8   99\n",
      "9  162\n",
      "\n",
      "\n",
      "\n",
      "Test truth after processing\n",
      "   id  max\n",
      "0   1  252\n",
      "1   2  192\n",
      "2   3  248\n",
      "3   4  283\n",
      "4   5  200\n",
      "5   6  224\n",
      "6   7  148\n",
      "7   8  262\n",
      "8   9  383\n",
      "9  10  185\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   id      248 non-null    int64\n",
      " 1   max     248 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.0 KB\n",
      "None\n",
      "Now we know the max RUL and how much RUL is left at each time step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test data after processing\n",
      "   id  cycle  setting1  setting2  setting3      s2       s3       s4      s7  \\\n",
      "0   1      1      20.0    0.7000     100.0  606.67  1481.04  1227.81  332.52   \n",
      "1   1      2      25.0    0.6200      60.0  536.22  1256.17  1031.48  174.46   \n",
      "2   1      3      42.0    0.8420     100.0  549.23  1340.13  1105.88  137.34   \n",
      "3   1      4      42.0    0.8402     100.0  549.19  1339.70  1107.26  137.23   \n",
      "4   1      5      35.0    0.8400     100.0  555.10  1353.04  1117.80  192.94   \n",
      "5   1      6      25.0    0.6203      60.0  536.40  1255.38  1043.74  174.32   \n",
      "6   1      7      20.0    0.7015     100.0  607.29  1473.03  1230.62  332.63   \n",
      "7   1      8      35.0    0.8414     100.0  554.81  1361.07  1115.14  193.45   \n",
      "8   1      9      20.0    0.7000     100.0  607.35  1481.89  1240.15  332.27   \n",
      "9   1     10      42.0    0.8416     100.0  549.30  1342.24  1106.32  136.65   \n",
      "\n",
      "        s8       s9    s11     s12      s13      s14      s15  s17    s20  \\\n",
      "0  2323.67  8704.98  43.83  313.03  2387.78  8048.98   9.2229  362  24.31   \n",
      "1  1915.21  7999.94  36.11  163.61  2028.09  7863.46  10.8632  306  14.36   \n",
      "2  2211.93  8305.38  41.52  129.98  2387.95  8071.13   9.3960  328  10.39   \n",
      "3  2211.89  8301.00  41.73  129.48  2387.90  8078.89   9.3594  328  10.56   \n",
      "4  2222.71  8331.05  41.32  181.82  2387.87  8057.83   9.3030  333  14.85   \n",
      "5  1915.27  8001.14  36.41  163.95  2028.12  7867.38  10.8661  304  14.17   \n",
      "6  2323.64  8708.79  43.94  312.77  2387.83  8052.97   9.1925  362  24.52   \n",
      "7  2222.76  8328.97  41.39  182.23  2387.84  8055.81   9.3004  331  14.84   \n",
      "8  2323.66  8705.64  43.79  312.20  2387.76  8052.46   9.1965  362  24.34   \n",
      "9  2211.85  8301.07  41.65  129.69  2387.97  8075.89   9.3860  328  10.45   \n",
      "\n",
      "       s21  \n",
      "0  14.7007  \n",
      "1   8.5748  \n",
      "2   6.4365  \n",
      "3   6.2367  \n",
      "4   8.9326  \n",
      "5   8.6135  \n",
      "6  14.6965  \n",
      "7   8.9004  \n",
      "8  14.6079  \n",
      "9   6.3830  \n",
      "\n",
      "\n",
      "\n",
      "Test y after processing\n",
      "     0\n",
      "0  251\n",
      "1  250\n",
      "2  249\n",
      "3  248\n",
      "4  247\n",
      "5  246\n",
      "6  245\n",
      "7  244\n",
      "8  243\n",
      "9  242\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Normalising the data\n",
      "bf\n",
      "train data\n",
      "   setting1  setting2  setting3      s2       s3       s4      s7       s8  \\\n",
      "0      42.0    0.8400     100.0  549.68  1343.43  1112.93  137.36  2211.86   \n",
      "1      20.0    0.7002     100.0  606.07  1477.61  1237.50  332.10  2323.66   \n",
      "2      42.0    0.8409     100.0  548.95  1343.12  1117.05  138.18  2211.92   \n",
      "3      42.0    0.8400     100.0  548.70  1341.24  1118.03  137.98  2211.88   \n",
      "4      25.0    0.6207      60.0  536.10  1255.23  1033.59  174.82  1915.22   \n",
      "5      35.0    0.8400     100.0  554.77  1352.87  1117.01  193.82  2222.77   \n",
      "6       0.0    0.0001     100.0  641.83  1583.47  1393.89  552.45  2387.92   \n",
      "7      42.0    0.8400     100.0  549.05  1344.16  1110.77  137.13  2211.92   \n",
      "8      42.0    0.8400     100.0  549.55  1342.85  1101.67  138.02  2211.90   \n",
      "9      25.0    0.6217      60.0  536.35  1251.91  1041.37  174.70  1915.23   \n",
      "\n",
      "        s9    s11     s12      s13      s14      s15  s17    s20      s21  \n",
      "0  8311.32  41.69  129.78  2387.99  8074.83   9.3335  330  10.62   6.3670  \n",
      "1  8713.60  43.94  312.59  2387.73  8046.13   9.1913  361  24.37  14.6552  \n",
      "2  8306.69  41.66  129.62  2387.97  8066.62   9.4007  329  10.48   6.4213  \n",
      "3  8312.35  41.68  129.80  2388.02  8076.05   9.3369  328  10.54   6.4176  \n",
      "4  7994.94  36.48  164.11  2028.08  7865.80  10.8366  305  14.03   8.6754  \n",
      "5  8340.00  41.44  181.90  2387.87  8054.10   9.3346  330  14.91   8.9057  \n",
      "6  9050.50  46.94  520.48  2387.89  8127.92   8.3960  391  38.93  23.4578  \n",
      "7  8307.28  41.60  129.65  2387.97  8075.99   9.3679  329  10.55   6.2787  \n",
      "8  8307.81  41.44  129.65  2388.00  8071.13   9.3384  328  10.63   6.3055  \n",
      "9  8005.83  36.24  164.08  2028.13  7869.41  10.9141  305  14.34   8.6119  \n",
      "\n",
      "test data\n",
      "   setting1  setting2  setting3      s2       s3       s4      s7       s8  \\\n",
      "0      20.0    0.7000     100.0  606.67  1481.04  1227.81  332.52  2323.67   \n",
      "1      25.0    0.6200      60.0  536.22  1256.17  1031.48  174.46  1915.21   \n",
      "2      42.0    0.8420     100.0  549.23  1340.13  1105.88  137.34  2211.93   \n",
      "3      42.0    0.8402     100.0  549.19  1339.70  1107.26  137.23  2211.89   \n",
      "4      35.0    0.8400     100.0  555.10  1353.04  1117.80  192.94  2222.71   \n",
      "5      25.0    0.6203      60.0  536.40  1255.38  1043.74  174.32  1915.27   \n",
      "6      20.0    0.7015     100.0  607.29  1473.03  1230.62  332.63  2323.64   \n",
      "7      35.0    0.8414     100.0  554.81  1361.07  1115.14  193.45  2222.76   \n",
      "8      20.0    0.7000     100.0  607.35  1481.89  1240.15  332.27  2323.66   \n",
      "9      42.0    0.8416     100.0  549.30  1342.24  1106.32  136.65  2211.85   \n",
      "\n",
      "        s9    s11     s12      s13      s14      s15  s17    s20      s21  \n",
      "0  8704.98  43.83  313.03  2387.78  8048.98   9.2229  362  24.31  14.7007  \n",
      "1  7999.94  36.11  163.61  2028.09  7863.46  10.8632  306  14.36   8.5748  \n",
      "2  8305.38  41.52  129.98  2387.95  8071.13   9.3960  328  10.39   6.4365  \n",
      "3  8301.00  41.73  129.48  2387.90  8078.89   9.3594  328  10.56   6.2367  \n",
      "4  8331.05  41.32  181.82  2387.87  8057.83   9.3030  333  14.85   8.9326  \n",
      "5  8001.14  36.41  163.95  2028.12  7867.38  10.8661  304  14.17   8.6135  \n",
      "6  8708.79  43.94  312.77  2387.83  8052.97   9.1925  362  24.52  14.6965  \n",
      "7  8328.97  41.39  182.23  2387.84  8055.81   9.3004  331  14.84   8.9004  \n",
      "8  8705.64  43.79  312.20  2387.76  8052.46   9.1965  362  24.34  14.6079  \n",
      "9  8301.07  41.65  129.69  2387.97  8075.89   9.3860  328  10.45   6.3830  \n",
      "\n",
      "\n",
      "Grouping by setting 1\n",
      "0.0\n",
      "10.0\n",
      "20.0\n",
      "25.0\n",
      "35.0\n",
      "42.0\n",
      "train_normalized is (61249, 14)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " After normalising the data\n",
      "train setting\n",
      "         s2        s3        s4        s7        s8        s9       s11  \\\n",
      "0  0.500000  0.255293  0.278866  0.152564  0.283276  0.141023  0.261905   \n",
      "1  0.113433  0.335117  0.217594  0.156289  0.114650  0.133347  0.241935   \n",
      "2  0.287791  0.249042  0.349656  0.257692  0.303754  0.113508  0.244048   \n",
      "3  0.215116  0.211131  0.366495  0.232051  0.290102  0.147144  0.255952   \n",
      "4  0.220641  0.320654  0.181548  0.237716  0.204918  0.086859  0.283871   \n",
      "\n",
      "        s12       s13       s14       s15       s17       s20       s21  \n",
      "0  0.206171  0.307937  0.175935  0.602638  0.363636  0.460000  0.473138  \n",
      "1  0.114089  0.059603  0.115529  0.577645  0.090909  0.297710  0.380397  \n",
      "2  0.183731  0.301587  0.125431  0.741138  0.272727  0.320000  0.564017  \n",
      "3  0.208976  0.317460  0.183440  0.609646  0.181818  0.380000  0.557824  \n",
      "4  0.230321  0.199219  0.171331  0.507187  0.333333  0.222222  0.625683  \n",
      "\n",
      "\n",
      "\n",
      "normalise target data so that it is between 0 and 1\n",
      "\n",
      "shape of test_y\n",
      "(41214, 1)\n",
      "number of train engine num :249\n",
      "\n",
      "\n",
      "Showing normalised training data\n",
      "         s2        s3        s4        s7        s8        s9       s11  \\\n",
      "0  0.500000  0.255293  0.278866  0.152564  0.283276  0.141023  0.261905   \n",
      "1  0.113433  0.335117  0.217594  0.156289  0.114650  0.133347  0.241935   \n",
      "2  0.287791  0.249042  0.349656  0.257692  0.303754  0.113508  0.244048   \n",
      "3  0.215116  0.211131  0.366495  0.232051  0.290102  0.147144  0.255952   \n",
      "4  0.220641  0.320654  0.181548  0.237716  0.204918  0.086859  0.283871   \n",
      "\n",
      "        s12       s13       s14       s15       s17       s20       s21  \n",
      "0  0.206171  0.307937  0.175935  0.602638  0.363636  0.460000  0.473138  \n",
      "1  0.114089  0.059603  0.115529  0.577645  0.090909  0.297710  0.380397  \n",
      "2  0.183731  0.301587  0.125431  0.741138  0.272727  0.320000  0.564017  \n",
      "3  0.208976  0.317460  0.183440  0.609646  0.181818  0.380000  0.557824  \n",
      "4  0.230321  0.199219  0.171331  0.507187  0.333333  0.222222  0.625683  \n",
      "\n",
      "\n",
      "showing train_x types of index 0\n",
      "[[0.5        0.2552934  0.278866   0.15256411 0.28327644 0.14102335\n",
      "  0.26190478 0.20617111 0.30793652 0.17593504 0.60263807 0.36363637\n",
      "  0.46       0.47313806]\n",
      " [0.11343284 0.33511704 0.21759425 0.15628892 0.11464968 0.1333475\n",
      "  0.24193548 0.11408935 0.05960265 0.11552925 0.5776454  0.09090909\n",
      "  0.2977099  0.38039657]\n",
      " [0.2877907  0.24904214 0.34965634 0.2576923  0.30375427 0.11350805\n",
      "  0.24404761 0.18373072 0.3015873  0.12543061 0.7411377  0.27272728\n",
      "  0.32       0.56401676]\n",
      " [0.21511628 0.21113127 0.36649483 0.23205128 0.2901024  0.14714447\n",
      "  0.2559524  0.20897616 0.31746033 0.18343996 0.6096455  0.18181819\n",
      "  0.38       0.55782425]\n",
      " [0.22064057 0.32065356 0.18154821 0.23771581 0.20491803 0.08685876\n",
      "  0.28387097 0.2303207  0.19921875 0.17133076 0.5071873  0.33333334\n",
      "  0.22222222 0.6256826 ]\n",
      " [0.20639534 0.19604689 0.25269023 0.22583927 0.1569966  0.11713039\n",
      "  0.17791411 0.14427313 0.1629393  0.13026221 0.6920211  0.09090909\n",
      "  0.4949495  0.40153715]\n",
      " [0.2845304  0.35084635 0.24056375 0.14205608 0.5714286  0.1821231\n",
      "  0.15476191 0.12634823 0.56790125 0.20083782 0.55449283 0.33333334\n",
      "  0.42857143 0.54342484]\n",
      " [0.31686047 0.2700141  0.24175258 0.12307692 0.30375427 0.11701432\n",
      "  0.20833333 0.18793829 0.3015873  0.18307087 0.67353666 0.27272728\n",
      "  0.39       0.32535565]\n",
      " [0.4622093  0.2435975  0.08539519 0.23717949 0.29692832 0.12016402\n",
      "  0.11309524 0.18793829 0.31111112 0.1531742  0.612737   0.18181819\n",
      "  0.47       0.37020922]\n",
      " [0.30960855 0.23589481 0.3355771  0.22177956 0.2090164  0.1775483\n",
      "  0.12903225 0.22594753 0.21875    0.20222507 0.6734606  0.33333334\n",
      "  0.5092593  0.51382774]\n",
      " [0.3552239  0.34425864 0.13806105 0.2260274  0.14012739 0.11221998\n",
      "  0.19892474 0.1395189  0.12582782 0.13975711 0.521301   0.27272728\n",
      "  0.49618322 0.3995984 ]\n",
      " [0.13662791 0.49758676 0.3626057  0.21057986 0.16040955 0.10215751\n",
      "  0.18404908 0.12334802 0.15974441 0.14355305 0.6251644  0.27272728\n",
      "  0.26262626 0.34944493]\n",
      " [0.29893237 0.3872862  0.32567808 0.1992032  0.22950819 0.12958027\n",
      "  0.2451613  0.16618076 0.22265625 0.16576807 0.51190734 0.33333334\n",
      "  0.5740741  0.5037872 ]\n",
      " [0.48656717 0.17279822 0.30574507 0.25653797 0.15286624 0.1373288\n",
      "  0.2311828  0.1628866  0.13907285 0.14764811 0.65987176 0.36363637\n",
      "  0.24427481 0.4497992 ]\n",
      " [0.227758   0.4174113  0.25975055 0.23904383 0.2090164  0.17013657\n",
      "  0.2516129  0.13119534 0.2109375  0.15361574 0.64492595 0.22222222\n",
      "  0.44444445 0.41377488]\n",
      " [0.13731343 0.25061315 0.3779174  0.2135741  0.13375796 0.15128994\n",
      "  0.18817204 0.1395189  0.14569536 0.14524382 0.5849748  0.09090909\n",
      "  0.4274809  0.4120231 ]\n",
      " [0.19283746 0.12038453 0.18923642 0.09192383 0.14285715 0.11312665\n",
      "  0.17073171 0.09269663 0.1147541  0.1633869  0.634696   0.36363637\n",
      "  0.34558824 0.470542  ]\n",
      " [0.4244186  0.28070176 0.29484537 0.16025642 0.3105802  0.12735485\n",
      "  0.26785713 0.2622721  0.30476192 0.20177165 0.6304617  0.27272728\n",
      "  0.46       0.48401675]\n",
      " [0.28656715 0.2173913  0.22190306 0.2117061  0.12101911 0.11466185\n",
      "  0.26344085 0.12714776 0.10596026 0.1538746  0.69285387 0.27272728\n",
      "  0.1221374  0.3128765 ]\n",
      " [0.22985074 0.32798216 0.17522442 0.24533    0.12101911 0.13350675\n",
      "  0.26344085 0.16563573 0.09271523 0.1623821  0.5758131  0.27272728\n",
      "  0.2977099  0.31789657]\n",
      " [0.23756906 0.20707847 0.30617204 0.07336449 0.20238096 0.08748393\n",
      "  0.27380952 0.06111967 0.19753087 0.16648713 0.56305057 0.25\n",
      "  0.4047619  0.3347933 ]\n",
      " [0.4358209  0.22630993 0.1653501  0.1618929  0.13375796 0.07405245\n",
      "  0.2204301  0.17731959 0.09271523 0.13741446 0.580623   0.27272728\n",
      "  0.35877863 0.5087851 ]\n",
      " [0.3038674  0.23279841 0.19893083 0.15093458 0.57738096 0.13952708\n",
      "  0.22023809 0.134566   0.5925926  0.23836026 0.5617921  0.25\n",
      "  0.44642857 0.50144213]\n",
      " [0.21352313 0.23640542 0.16689764 0.24701196 0.22131148 0.11808794\n",
      "  0.2580645  0.1122449  0.22265625 0.18605049 0.6140313  0.11111111\n",
      "  0.4537037  0.38576713]\n",
      " [0.3372093  0.23008671 0.31477663 0.34358975 0.28668943 0.10922921\n",
      "  0.26785713 0.24824685 0.2984127  0.15526575 0.5857378  0.27272728\n",
      "  0.3        0.4023431 ]\n",
      " [0.15522388 0.3270903  0.29174146 0.20485678 0.15923567 0.1010723\n",
      "  0.12365592 0.15120275 0.12582782 0.18297268 0.60765004 0.27272728\n",
      "  0.32061067 0.3190261 ]\n",
      " [0.2506887  0.3917413  0.15656124 0.10965201 0.16806723 0.13846746\n",
      "  0.25       0.15870787 0.1147541  0.14960583 0.6040356  0.27272728\n",
      "  0.40441176 0.40231174]\n",
      " [0.48618785 0.21587162 0.32415357 0.07990655 0.22619048 0.10470121\n",
      "  0.29166666 0.02927581 0.25308642 0.14159186 0.62698215 0.25\n",
      "  0.30952382 0.44749492]\n",
      " [0.16014235 0.14679602 0.3242922  0.23240373 0.21311475 0.18712525\n",
      "  0.30322582 0.13702624 0.203125   0.16593924 0.5713366  0.33333334\n",
      "  0.5648148  0.38946626]\n",
      " [0.27462685 0.22162765 0.22639138 0.18119551 0.10828026 0.09693173\n",
      "  0.21505377 0.1347079  0.1192053  0.15923803 0.63055426 0.18181819\n",
      "  0.35877863 0.4572038 ]\n",
      " [0.39104477 0.44793758 0.16696589 0.19613947 0.14649682 0.11020278\n",
      "  0.18817204 0.14776632 0.1192053  0.17705444 0.66422355 0.09090909\n",
      "  0.35114503 0.47126004]\n",
      " [0.23546511 0.12617789 0.26902384 0.22075279 0.17064847 0.15367837\n",
      "  0.11042945 0.21365638 0.17891374 0.1804787  0.6587023  0.18181819\n",
      "  0.7070707  0.44594362]\n",
      " [0.46802327 0.24904214 0.22336769 0.13205129 0.28668943 0.14744161\n",
      "  0.27380952 0.23422159 0.30793652 0.19352855 0.58882934 0.18181819\n",
      "  0.38       0.31514645]\n",
      " [0.16860466 0.1532981  0.36548808 0.1688708  0.1569966  0.08960151\n",
      "  0.13496932 0.20264317 0.1629393  0.1550397  0.6799649  0.36363637\n",
      "  0.47474748 0.46046114]\n",
      " [0.29752067 0.27987766 0.23571554 0.15955351 0.1764706  0.14341469\n",
      "  0.16463415 0.16151686 0.13934426 0.16314618 0.49109015 0.27272728\n",
      "  0.31617647 0.48086634]\n",
      " [0.22965117 0.17624521 0.2530928  0.1974359  0.2764505  0.06733226\n",
      "  0.24404761 0.2230014  0.30476192 0.15569636 0.604493   0.27272728\n",
      "  0.42       0.37221757]\n",
      " [0.40697673 0.28545162 0.21944658 0.15666327 0.1569966  0.08229191\n",
      "  0.12883435 0.1431718  0.16613418 0.12713495 0.5725559  0.27272728\n",
      "  0.5252525  0.34210077]\n",
      " [0.40607736 0.4036052  0.37356228 0.04205608 0.03571429 0.08904914\n",
      "  0.44642857 0.05495634 0.00617284 0.16020347 0.6800906  0.25\n",
      "  0.41666666 0.41416514]\n",
      " [0.21802326 0.4215123  0.29342812 0.15564598 0.17406143 0.122141\n",
      "  0.2208589  0.18392071 0.15654951 0.14704114 0.7005699  0.27272728\n",
      "  0.5050505  0.34244236]\n",
      " [0.24709302 0.31280166 0.12970792 0.26653102 0.15358362 0.11489036\n",
      "  0.20858896 0.1585903  0.17252396 0.13958384 0.7025427  0.27272728\n",
      "  0.5252525  0.41383433]\n",
      " [0.38078293 0.30150625 0.24886161 0.19787517 0.20491803 0.14482012\n",
      "  0.21935484 0.25801748 0.22265625 0.1228926  0.6153186  0.33333334\n",
      "  0.6203704  0.5340849 ]\n",
      " [0.27046263 0.43757978 0.2193625  0.21646747 0.25       0.11833777\n",
      "  0.32258064 0.26239067 0.23828125 0.1741549  0.56275475 0.22222222\n",
      "  0.44444445 0.3271094 ]\n",
      " [0.29537368 0.34133264 0.08651752 0.16069058 0.21311475 0.07928048\n",
      "  0.2451613  0.1909621  0.23828125 0.17252888 0.5462347  0.33333334\n",
      "  0.5648148  0.52527744]\n",
      " [0.13167259 0.17207046 0.29063553 0.20053121 0.20081967 0.11767155\n",
      "  0.19354838 0.09475219 0.20703125 0.16747968 0.6125295  0.44444445\n",
      "  0.44444445 0.44900477]\n",
      " [0.33096084 0.29333675 0.35359335 0.19389111 0.20081967 0.1782978\n",
      "  0.16129032 0.16909622 0.22265625 0.17107403 0.55438745 0.44444445\n",
      "  0.39814815 0.48652458]\n",
      " [0.5718232  0.21015608 0.42037907 0.08130841 0.2797619  0.10386271\n",
      "  0.27380952 0.06368773 0.2962963  0.15918611 0.670526   0.16666667\n",
      "  0.44047618 0.24687533]\n",
      " [0.519573   0.11998979 0.34270442 0.30278885 0.21311475 0.10559627\n",
      "  0.19354838 0.09620991 0.2109375  0.19897304 0.5717657  0.44444445\n",
      "  0.41666666 0.48758146]\n",
      " [0.3866279  0.2353297  0.24570447 0.21153846 0.3003413  0.10780293\n",
      "  0.24404761 0.2580645  0.3015873  0.14985237 0.63190436 0.27272728\n",
      "  0.35       0.4823431 ]\n",
      " [0.3044776  0.29632106 0.19443446 0.18991283 0.14012739 0.11545812\n",
      "  0.16666667 0.14295533 0.12582782 0.13365391 0.6335318  0.36363637\n",
      "  0.38931298 0.48418674]\n",
      " [0.30813953 0.1990347  0.21176018 0.19735503 0.18430035 0.09007309\n",
      "  0.17177914 0.11563877 0.15335463 0.19010103 0.6021482  0.09090909\n",
      "  0.42424244 0.5385141 ]]\n",
      "\n",
      "\n",
      "showing train_x types of index 1\n",
      "[[0.11343284 0.33511704 0.21759425 0.15628892 0.11464968 0.1333475\n",
      "  0.24193548 0.11408935 0.05960265 0.11552925 0.5776454  0.09090909\n",
      "  0.2977099  0.38039657]\n",
      " [0.2877907  0.24904214 0.34965634 0.2576923  0.30375427 0.11350805\n",
      "  0.24404761 0.18373072 0.3015873  0.12543061 0.7411377  0.27272728\n",
      "  0.32       0.56401676]\n",
      " [0.21511628 0.21113127 0.36649483 0.23205128 0.2901024  0.14714447\n",
      "  0.2559524  0.20897616 0.31746033 0.18343996 0.6096455  0.18181819\n",
      "  0.38       0.55782425]\n",
      " [0.22064057 0.32065356 0.18154821 0.23771581 0.20491803 0.08685876\n",
      "  0.28387097 0.2303207  0.19921875 0.17133076 0.5071873  0.33333334\n",
      "  0.22222222 0.6256826 ]\n",
      " [0.20639534 0.19604689 0.25269023 0.22583927 0.1569966  0.11713039\n",
      "  0.17791411 0.14427313 0.1629393  0.13026221 0.6920211  0.09090909\n",
      "  0.4949495  0.40153715]\n",
      " [0.2845304  0.35084635 0.24056375 0.14205608 0.5714286  0.1821231\n",
      "  0.15476191 0.12634823 0.56790125 0.20083782 0.55449283 0.33333334\n",
      "  0.42857143 0.54342484]\n",
      " [0.31686047 0.2700141  0.24175258 0.12307692 0.30375427 0.11701432\n",
      "  0.20833333 0.18793829 0.3015873  0.18307087 0.67353666 0.27272728\n",
      "  0.39       0.32535565]\n",
      " [0.4622093  0.2435975  0.08539519 0.23717949 0.29692832 0.12016402\n",
      "  0.11309524 0.18793829 0.31111112 0.1531742  0.612737   0.18181819\n",
      "  0.47       0.37020922]\n",
      " [0.30960855 0.23589481 0.3355771  0.22177956 0.2090164  0.1775483\n",
      "  0.12903225 0.22594753 0.21875    0.20222507 0.6734606  0.33333334\n",
      "  0.5092593  0.51382774]\n",
      " [0.3552239  0.34425864 0.13806105 0.2260274  0.14012739 0.11221998\n",
      "  0.19892474 0.1395189  0.12582782 0.13975711 0.521301   0.27272728\n",
      "  0.49618322 0.3995984 ]\n",
      " [0.13662791 0.49758676 0.3626057  0.21057986 0.16040955 0.10215751\n",
      "  0.18404908 0.12334802 0.15974441 0.14355305 0.6251644  0.27272728\n",
      "  0.26262626 0.34944493]\n",
      " [0.29893237 0.3872862  0.32567808 0.1992032  0.22950819 0.12958027\n",
      "  0.2451613  0.16618076 0.22265625 0.16576807 0.51190734 0.33333334\n",
      "  0.5740741  0.5037872 ]\n",
      " [0.48656717 0.17279822 0.30574507 0.25653797 0.15286624 0.1373288\n",
      "  0.2311828  0.1628866  0.13907285 0.14764811 0.65987176 0.36363637\n",
      "  0.24427481 0.4497992 ]\n",
      " [0.227758   0.4174113  0.25975055 0.23904383 0.2090164  0.17013657\n",
      "  0.2516129  0.13119534 0.2109375  0.15361574 0.64492595 0.22222222\n",
      "  0.44444445 0.41377488]\n",
      " [0.13731343 0.25061315 0.3779174  0.2135741  0.13375796 0.15128994\n",
      "  0.18817204 0.1395189  0.14569536 0.14524382 0.5849748  0.09090909\n",
      "  0.4274809  0.4120231 ]\n",
      " [0.19283746 0.12038453 0.18923642 0.09192383 0.14285715 0.11312665\n",
      "  0.17073171 0.09269663 0.1147541  0.1633869  0.634696   0.36363637\n",
      "  0.34558824 0.470542  ]\n",
      " [0.4244186  0.28070176 0.29484537 0.16025642 0.3105802  0.12735485\n",
      "  0.26785713 0.2622721  0.30476192 0.20177165 0.6304617  0.27272728\n",
      "  0.46       0.48401675]\n",
      " [0.28656715 0.2173913  0.22190306 0.2117061  0.12101911 0.11466185\n",
      "  0.26344085 0.12714776 0.10596026 0.1538746  0.69285387 0.27272728\n",
      "  0.1221374  0.3128765 ]\n",
      " [0.22985074 0.32798216 0.17522442 0.24533    0.12101911 0.13350675\n",
      "  0.26344085 0.16563573 0.09271523 0.1623821  0.5758131  0.27272728\n",
      "  0.2977099  0.31789657]\n",
      " [0.23756906 0.20707847 0.30617204 0.07336449 0.20238096 0.08748393\n",
      "  0.27380952 0.06111967 0.19753087 0.16648713 0.56305057 0.25\n",
      "  0.4047619  0.3347933 ]\n",
      " [0.4358209  0.22630993 0.1653501  0.1618929  0.13375796 0.07405245\n",
      "  0.2204301  0.17731959 0.09271523 0.13741446 0.580623   0.27272728\n",
      "  0.35877863 0.5087851 ]\n",
      " [0.3038674  0.23279841 0.19893083 0.15093458 0.57738096 0.13952708\n",
      "  0.22023809 0.134566   0.5925926  0.23836026 0.5617921  0.25\n",
      "  0.44642857 0.50144213]\n",
      " [0.21352313 0.23640542 0.16689764 0.24701196 0.22131148 0.11808794\n",
      "  0.2580645  0.1122449  0.22265625 0.18605049 0.6140313  0.11111111\n",
      "  0.4537037  0.38576713]\n",
      " [0.3372093  0.23008671 0.31477663 0.34358975 0.28668943 0.10922921\n",
      "  0.26785713 0.24824685 0.2984127  0.15526575 0.5857378  0.27272728\n",
      "  0.3        0.4023431 ]\n",
      " [0.15522388 0.3270903  0.29174146 0.20485678 0.15923567 0.1010723\n",
      "  0.12365592 0.15120275 0.12582782 0.18297268 0.60765004 0.27272728\n",
      "  0.32061067 0.3190261 ]\n",
      " [0.2506887  0.3917413  0.15656124 0.10965201 0.16806723 0.13846746\n",
      "  0.25       0.15870787 0.1147541  0.14960583 0.6040356  0.27272728\n",
      "  0.40441176 0.40231174]\n",
      " [0.48618785 0.21587162 0.32415357 0.07990655 0.22619048 0.10470121\n",
      "  0.29166666 0.02927581 0.25308642 0.14159186 0.62698215 0.25\n",
      "  0.30952382 0.44749492]\n",
      " [0.16014235 0.14679602 0.3242922  0.23240373 0.21311475 0.18712525\n",
      "  0.30322582 0.13702624 0.203125   0.16593924 0.5713366  0.33333334\n",
      "  0.5648148  0.38946626]\n",
      " [0.27462685 0.22162765 0.22639138 0.18119551 0.10828026 0.09693173\n",
      "  0.21505377 0.1347079  0.1192053  0.15923803 0.63055426 0.18181819\n",
      "  0.35877863 0.4572038 ]\n",
      " [0.39104477 0.44793758 0.16696589 0.19613947 0.14649682 0.11020278\n",
      "  0.18817204 0.14776632 0.1192053  0.17705444 0.66422355 0.09090909\n",
      "  0.35114503 0.47126004]\n",
      " [0.23546511 0.12617789 0.26902384 0.22075279 0.17064847 0.15367837\n",
      "  0.11042945 0.21365638 0.17891374 0.1804787  0.6587023  0.18181819\n",
      "  0.7070707  0.44594362]\n",
      " [0.46802327 0.24904214 0.22336769 0.13205129 0.28668943 0.14744161\n",
      "  0.27380952 0.23422159 0.30793652 0.19352855 0.58882934 0.18181819\n",
      "  0.38       0.31514645]\n",
      " [0.16860466 0.1532981  0.36548808 0.1688708  0.1569966  0.08960151\n",
      "  0.13496932 0.20264317 0.1629393  0.1550397  0.6799649  0.36363637\n",
      "  0.47474748 0.46046114]\n",
      " [0.29752067 0.27987766 0.23571554 0.15955351 0.1764706  0.14341469\n",
      "  0.16463415 0.16151686 0.13934426 0.16314618 0.49109015 0.27272728\n",
      "  0.31617647 0.48086634]\n",
      " [0.22965117 0.17624521 0.2530928  0.1974359  0.2764505  0.06733226\n",
      "  0.24404761 0.2230014  0.30476192 0.15569636 0.604493   0.27272728\n",
      "  0.42       0.37221757]\n",
      " [0.40697673 0.28545162 0.21944658 0.15666327 0.1569966  0.08229191\n",
      "  0.12883435 0.1431718  0.16613418 0.12713495 0.5725559  0.27272728\n",
      "  0.5252525  0.34210077]\n",
      " [0.40607736 0.4036052  0.37356228 0.04205608 0.03571429 0.08904914\n",
      "  0.44642857 0.05495634 0.00617284 0.16020347 0.6800906  0.25\n",
      "  0.41666666 0.41416514]\n",
      " [0.21802326 0.4215123  0.29342812 0.15564598 0.17406143 0.122141\n",
      "  0.2208589  0.18392071 0.15654951 0.14704114 0.7005699  0.27272728\n",
      "  0.5050505  0.34244236]\n",
      " [0.24709302 0.31280166 0.12970792 0.26653102 0.15358362 0.11489036\n",
      "  0.20858896 0.1585903  0.17252396 0.13958384 0.7025427  0.27272728\n",
      "  0.5252525  0.41383433]\n",
      " [0.38078293 0.30150625 0.24886161 0.19787517 0.20491803 0.14482012\n",
      "  0.21935484 0.25801748 0.22265625 0.1228926  0.6153186  0.33333334\n",
      "  0.6203704  0.5340849 ]\n",
      " [0.27046263 0.43757978 0.2193625  0.21646747 0.25       0.11833777\n",
      "  0.32258064 0.26239067 0.23828125 0.1741549  0.56275475 0.22222222\n",
      "  0.44444445 0.3271094 ]\n",
      " [0.29537368 0.34133264 0.08651752 0.16069058 0.21311475 0.07928048\n",
      "  0.2451613  0.1909621  0.23828125 0.17252888 0.5462347  0.33333334\n",
      "  0.5648148  0.52527744]\n",
      " [0.13167259 0.17207046 0.29063553 0.20053121 0.20081967 0.11767155\n",
      "  0.19354838 0.09475219 0.20703125 0.16747968 0.6125295  0.44444445\n",
      "  0.44444445 0.44900477]\n",
      " [0.33096084 0.29333675 0.35359335 0.19389111 0.20081967 0.1782978\n",
      "  0.16129032 0.16909622 0.22265625 0.17107403 0.55438745 0.44444445\n",
      "  0.39814815 0.48652458]\n",
      " [0.5718232  0.21015608 0.42037907 0.08130841 0.2797619  0.10386271\n",
      "  0.27380952 0.06368773 0.2962963  0.15918611 0.670526   0.16666667\n",
      "  0.44047618 0.24687533]\n",
      " [0.519573   0.11998979 0.34270442 0.30278885 0.21311475 0.10559627\n",
      "  0.19354838 0.09620991 0.2109375  0.19897304 0.5717657  0.44444445\n",
      "  0.41666666 0.48758146]\n",
      " [0.3866279  0.2353297  0.24570447 0.21153846 0.3003413  0.10780293\n",
      "  0.24404761 0.2580645  0.3015873  0.14985237 0.63190436 0.27272728\n",
      "  0.35       0.4823431 ]\n",
      " [0.3044776  0.29632106 0.19443446 0.18991283 0.14012739 0.11545812\n",
      "  0.16666667 0.14295533 0.12582782 0.13365391 0.6335318  0.36363637\n",
      "  0.38931298 0.48418674]\n",
      " [0.30813953 0.1990347  0.21176018 0.19735503 0.18430035 0.09007309\n",
      "  0.17177914 0.11563877 0.15335463 0.19010103 0.6021482  0.09090909\n",
      "  0.42424244 0.5385141 ]\n",
      " [0.27823693 0.42560628 0.37008563 0.13854235 0.1092437  0.16457784\n",
      "  0.22560975 0.19662921 0.09016393 0.17548294 0.49449685 0.27272728\n",
      "  0.3602941  0.44708785]]\n",
      "\n",
      " number of nested array in train_x\n",
      "49048\n",
      "\n",
      "\n",
      "train_y types before training\n",
      "      id  RUL\n",
      "0  0.008  1.0\n",
      "1  0.008  1.0\n",
      "2  0.008  1.0\n",
      "3  0.008  1.0\n",
      "4  0.008  1.0\n",
      "\n",
      "\n",
      "showing train_y types\n",
      "(49048, 2)\n",
      "[0.008 1.   ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(49048, 50, 14)\n",
      "(248, 50, 14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CMAPSS():\n",
    "    def __init__(self, data_root, data_set, max_rul, seq_len):\n",
    "        # load params\n",
    "        self.data_root = data_root  # path to data directory\n",
    "        self.data_set = data_set  # name of data set\n",
    "        self.max_rul = max_rul   # max RUL value cap ?\n",
    "        self.seq_len = seq_len\n",
    "        self.column_names = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                             's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                             's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "        self.mode = None\n",
    "        self.val_fold = 0\n",
    "\n",
    "        # load CMAPSS_data\n",
    "        self.train_data_df, self.test_data_df, self.test_truth = self._get_data(data_root=data_root, data_set=data_set) \n",
    "        # helper function (get data)  to load raw data from txt files\n",
    "\n",
    "\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = self._process(self.train_data_df, self.test_data_df, self.test_truth)\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        print(np.array(self.train_x).shape)\n",
    "        print(np.array(self.test_x).shape)\n",
    "\n",
    "        self.data_save()\n",
    "\n",
    "\n",
    "    def _get_data(self, data_root, data_set):\n",
    "        # helper function to load raw data from txt files from my local directory\n",
    "\n",
    "        train_data_pt = os.path.join(data_root, 'CMAPSSData', 'train_' + data_set + '.txt')\n",
    "        assert os.path.exists(train_data_pt), 'data path does not exist: {:}'.format(train_data_pt)\n",
    "        # print(train_data_pt)\n",
    "        test_data_pt = os.path.join(data_root, 'CMAPSSData', 'test_' + data_set + '.txt')\n",
    "        assert os.path.exists(test_data_pt), 'data path does not exist: {:}'.format(test_data_pt)\n",
    "\n",
    "        test_truth_pt = os.path.join(data_root, 'CMAPSSData', 'RUL_' + data_set + '.txt')\n",
    "        assert os.path.exists(test_truth_pt), 'data path does not exist: {:}'.format(test_truth_pt)\n",
    "\n",
    "        train_data_df = pd.read_csv(train_data_pt, sep=\" \", header=None)\n",
    "        train_data_df.drop(train_data_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "        train_data_df.columns = self.column_names\n",
    "        train_data_df = train_data_df.sort_values(['id', 'cycle'])\n",
    "\n",
    "        test_data_df = pd.read_csv(test_data_pt, sep=\" \", header=None)\n",
    "        test_data_df.drop(test_data_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "        test_data_df.columns = self.column_names\n",
    "        test_data_df = test_data_df.sort_values(['id', 'cycle'])\n",
    "\n",
    "        test_truth = pd.read_csv(test_truth_pt, sep=\" \", header=None)\n",
    "        test_truth.drop(test_truth.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"seeing the stucture of the orginal data\")\n",
    "        print(\"Training Data Summary:\")\n",
    "        print(train_data_df.info())\n",
    "\n",
    "        rows, columns = train_data_df.shape\n",
    "        print(f\"Number of rows/ time steps: {rows}\")\n",
    "        print(f\"Number of columns: {columns}\")\n",
    "        print(train_data_df.head(6))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        print(\"\\nTest Data Summary:\")\n",
    "        print(test_data_df.info())\n",
    "   \n",
    "        rows, columns = test_data_df.shape\n",
    "        print(f\"Number of rows/ time steps: {rows}\")\n",
    "        print(f\"Number of columns: {columns}\")\n",
    "        print(test_data_df.head(6))\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"Test Truth Summary:\")\n",
    "        print(test_truth.info())\n",
    "        print(test_truth.head(6))\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "\n",
    "        print(\"\\n\\n\" + \"We can see how magnitude of difference sensor data can vary -> thats why we need to norlamize the data later\")\n",
    "\n",
    "  \n",
    "        return train_data_df, test_data_df, test_truth\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _process(self, train_df, test_df, test_truth):\n",
    "\n",
    "        print(\"\\n\\n\\n\\n\\nNow the processing data function\")\n",
    "        # process train data\n",
    "        # to find the maximum cycle for each engine and merge it with the train data\n",
    "        # as well as calculate the RUL at each time step\n",
    "        train_rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index() # cycle there means time step \n",
    "        train_rul.columns = ['id', 'max']  # add column name\n",
    "\n",
    "        print(\"train_rul\" + \" visualization\\n\")\n",
    "        print(\"As we found already for each id the max cycle\")\n",
    "        print(train_rul.head(7))\n",
    "\n",
    "\n",
    "\n",
    "        train_df = train_df.merge(train_rul, on=['id'], how='left')  # merge the max column to the train data\n",
    "        # calculate the RUL at each time step\n",
    "        print(\"\\nAfter merging with the max cycle for corresponding id\")\n",
    "        print(train_df.head())\n",
    "        print(\"\\nWe now also have access to the actual RUL that it will decay for each machine, at their own timestep\\n\")\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "        # drop the max as not needed anymore\n",
    "        train_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "        # drop some columns of sensor (  Some sensors may not be directly affected by degradation mode ???) maybe these sensor are not important factors to RUL\n",
    "        train_df.drop(['s1', 's5', 's6', 's10', 's16', 's18', 's19'], axis=1, inplace=True)\n",
    "        print(\"we remove sensors with constant values , specifically those with sensor indices 1,5,6,10,16,18,and 19.\")\n",
    "        # This dataset includes four subdata sets from FD001 to FD004,each collected under different operating conditions and faultmodes.\n",
    "\n",
    "        print(\"\\n\\n\\n\\n\\nNow used the max to cal the RUL at each time step\")\n",
    "        print(\"max is then removed from the data\")\n",
    "        print(\"We can now observe that some sensor data are removed\")\n",
    "        print(train_df.head(10))\n",
    "\n",
    "  \n",
    "        train_df['setting1'] = train_df['setting1'].round(1) # as its the only data with many decimal places\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # calculate the RUL at each time step\n",
    "        train_y = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "        train_y.columns = ['id', 'max']\n",
    "        train_y = train_df.merge(train_y, on=['id'], how='left')\n",
    "        train_y['RUL'] = train_y['max'] - train_y['cycle']\n",
    "        train_y = train_y[['id', 'RUL']]\n",
    "        train_y = train_y.apply(lambda x: [y if y <= self.max_rul else self.max_rul for y in x])\n",
    "        train_engine_num = train_df['id'].nunique()\n",
    "        logging.info(\"CMPDataIter:: iterator initialized (train engine number: {:})\".format(train_engine_num))  \n",
    "        print(\"Normalising of training data\")\n",
    "        print(\"Get the max cycle for each engine and calculate the RUL at each time step, also cappingt the max rul at self defined maximal RUL\")\n",
    "        print(\"Using max - current cycle = RUL remaining at each time step\")\n",
    "        print(\"Therefore getting our y labels for training data\")\n",
    "        print(train_y.head(10))\n",
    "        print(\"we can see that all RUL is 125, this is because it's RUL is capped at 125, the acutal RUL is actually more than 125\")\n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Now processing the test data\")\n",
    "        # process test data\n",
    "        # We dont know when the test engine will fail\n",
    "        print(\"Test data summary before processing\")\n",
    "        print(test_df.info())\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"test truth summary before processing\")\n",
    "        print(test_truth.info())\n",
    "        print(test_truth.head(10))\n",
    "\n",
    "\n",
    "        # find the max cycle on the testing data\n",
    "        test_rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "        test_rul.columns = ['id', 'max'] # get the max cycle/ time step -> last cycle time step\n",
    "\n",
    "\n",
    "        # the test_truth is our ground truth for the test data\n",
    "        # test truth more column is the RUL at each time step\n",
    "        test_truth.columns = ['more'] # add column name\n",
    "        test_truth['id'] = test_truth.index + 1  # adjusting the row index to start from 1 instead of 0\n",
    "        test_truth['max'] = test_rul['max'] + test_truth['more']  # (predicted final cycle number) = max cycle + RUL at each time step to get the max RUL \n",
    "        # so max there is the actual max RUL of the engine at cycle 0, as they didint provide final cycle as test data\n",
    "        #The testing file only contains the sensor measurements to certain running cycles for another certain number of engines\n",
    "\n",
    "        # repeat the process\n",
    "        test_truth.drop('more', axis=1, inplace= True)\n",
    "\n",
    "        print(\"\\n\\n\\nTest truth after processing\")\n",
    "        print(test_truth.head(10))\n",
    "        print(test_truth.info())\n",
    "        print(\"Now we know the max RUL and how much RUL is left at each time step\")\n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "\n",
    "\n",
    "        test_df = test_df.merge(test_truth, on=['id'], how='left')\n",
    "        test_y = pd.DataFrame(data=[test_df['max'] - test_df['cycle']]).T # now we have the RUL at each time step\n",
    "\n",
    "        test_df.drop('max', axis=1, inplace=True)\n",
    "        test_df.drop(['s1', 's5', 's6', 's10', 's16', 's18', 's19'], axis=1, inplace=True)\n",
    "\n",
    "        test_df['setting1'] = test_df['setting1'].round(1)\n",
    "\n",
    "        print(\"\\n\\n\\nTest data after processing\")\n",
    "        print(test_df.head(10))\n",
    "\n",
    "        print(\"\\n\\n\\nTest y after processing\")\n",
    "        print(test_y.head(10))\n",
    "\n",
    "        test_y = test_y.apply(lambda x: [y if y <= self.max_rul else self.max_rul for y in x])\n",
    "        test_engine_num = test_df['id'].nunique()\n",
    "        logging.info(\"CMPDataIter:: iterator initialized (test engine number: {:})\".format(test_engine_num))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "        # normailize both train and test data to setting 1\n",
    "        # this selects the relevant feature columns (excluding id and cycle) from the dataset\n",
    "        train_data = train_df.iloc[:, 2:]\n",
    "        test_data = test_df.iloc[:, 2:]\n",
    "\n",
    "        print(\"\\n\\n\\n\\nNormalising the data\")\n",
    "        print(\"bf\")\n",
    "        print(\"train data\")\n",
    "        print(train_data.head(10))\n",
    "        print(\"\\ntest data\")\n",
    "        print(test_data.head(10))\n",
    "\n",
    "        # create a new dataframe to store the normalized data\n",
    "        train_normalized = pd.DataFrame(columns=train_data.columns[3:])\n",
    "        test_normalized = pd.DataFrame(columns=test_data.columns[3:])\n",
    "\n",
    "\n",
    "        scaler = MinMaxScaler() # helps ensure that features with larger ranges don't dominate the model's training\n",
    "\n",
    "        grouped_train = train_data.groupby('setting1')\n",
    "        grouped_test = test_data.groupby('setting1')\n",
    "        #  Grouping by setting1 allows you to normalize data specific to each operating condition, preserving the distribution of features within each group.\n",
    "\n",
    "        print(\"\\n\\nGrouping by setting 1\") # it iterate per ascending order of setting 1 index\n",
    "        # print(grouped_train.head(6))\n",
    "\n",
    "        # question: since setting 1 or 2 both exist, how can we just group it by setting 1 ??\n",
    "\n",
    "\n",
    "        for train_idx, train in grouped_train: # group by setting1, so each train_idx might hv multiple train data rows\n",
    "\n",
    "            print(train_idx) # to it will show train as a batch for its corresponding index \n",
    "\n",
    "            scaled_train = scaler.fit_transform(train.iloc[:, 3:]) # scale by each setting to make normalization consistent\n",
    "            scaled_train_combine = pd.DataFrame(\n",
    "                data=scaled_train,\n",
    "                index=train.index,\n",
    "                columns=train_data.columns[3:])\n",
    "            train_normalized = pd.concat([train_normalized, scaled_train_combine])\n",
    "\n",
    "\n",
    "            # after normalizing the training data, we normalize the test data too\n",
    "            for test_idx, test in grouped_test: # for loop to find the right data setting\n",
    "                # normalize the test data based on their setting\n",
    "                if train_idx == test_idx:\n",
    "                    scaled_test = scaler.transform(test.iloc[:, 3:])\n",
    "                    scaled_test_combine = pd.DataFrame(\n",
    "                        data=scaled_test,\n",
    "                        index=test.index,\n",
    "                        columns=test_data.columns[3:])\n",
    "                    test_normalized = pd.concat([test_normalized, scaled_test_combine])\n",
    "\n",
    "\n",
    "\n",
    "        train_normalized = train_normalized.sort_index()\n",
    "        test_normalized = test_normalized.sort_index()\n",
    "        print('train_normalized is '+ str(np.shape(train_normalized)))\n",
    "        # diff@xuqing\n",
    "\n",
    "        # normalize the setting data as well\n",
    "        train_setting = scaler.fit_transform(train_df.iloc[:, 1:5])\n",
    "        test_setting = scaler.transform(test_df.iloc[:, 1:5])\n",
    "\n",
    "\n",
    "\n",
    "        train_setting = pd.DataFrame(\n",
    "            data=train_setting,\n",
    "            index=train_df.index,\n",
    "            columns=train_df.columns[1:5])\n",
    "\n",
    "        test_setting = pd.DataFrame(\n",
    "            data=test_setting,\n",
    "            index=test_df.index,\n",
    "            columns=test_df.columns[1:5])\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n After normalising the data\")\n",
    "\n",
    "        print(\"train setting\")\n",
    "        print(train_normalized.head(5))\n",
    "        \n",
    "\n",
    "        # normalize the target data as well\n",
    "        print(\"\\n\\n\\nnormalise target data so that it is between 0 and 1\")\n",
    "        train_y = train_y.apply(lambda x: (x / self.max_rul))  # norm_y = y/max_RUL\n",
    "        test_y = test_y.apply(lambda x: (x / self.max_rul))  # norm_y = y/max_RUL\n",
    "        \n",
    "        \n",
    "        print(\"\\nshape of test_y\")\n",
    "        print(np.shape(test_y))\n",
    "\n",
    "\n",
    "        condition_num = train_df['setting1'].nunique()\n",
    "\n",
    "        if condition_num > 1:\n",
    "            logging.info(\"CMPDataIter:: data includes multi operating conditions\")\n",
    "        else:\n",
    "            logging.info(\"CMPDataIter:: data includes single operating conditions\")\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "        # generate final data:\n",
    "        # generate 9 x 15 windows to obtain train_x , 15 from the sliding window size\n",
    "        seq_gen = []\n",
    "        start_index = 0\n",
    "        for i in range(train_engine_num):\n",
    "            end_index = start_index + train_rul.loc[i, 'max']\n",
    "            # print(\"\\nCurrent end index: \" + str(end_index))\n",
    "\n",
    "            if end_index - start_index < self.seq_len - 1:\n",
    "                print('train data less than seq_len!')\n",
    "            # for sensor train matrix, number of 21 X 15 needed per data points (minus the first sequence length) per engine, so the array input start from start index\n",
    "            val = list(self.gen_sequence(train_normalized.iloc[start_index:end_index, :], self.seq_len,\n",
    "                                         train_normalized.columns))\n",
    "            seq_gen.extend(val)\n",
    "            start_index = end_index\n",
    "        train_x = list(seq_gen)\n",
    "\n",
    "        print(\"number of train engine num :\" + str(train_engine_num))\n",
    "\n",
    "        # generate 3 x 15 windows to obtain train_ops\n",
    "        seq_gen = []\n",
    "        start_index = 0\n",
    "        for i in range(train_engine_num):\n",
    "            end_index = start_index + train_rul.loc[i, 'max']\n",
    "            # print(end_index)\n",
    "            # for ops train matrix, number of 3 X 15 needed per data points (minus the first sequence length) per engine, so the array input start from start index\n",
    "            # settings data are in the first 3 columns of Train_Norm\n",
    "            val = list(\n",
    "                self.gen_sequence(train_setting.iloc[start_index:end_index, :], self.seq_len, train_setting.columns))\n",
    "            seq_gen.extend(val)\n",
    "            start_index = end_index\n",
    "        train_ops = list(seq_gen)\n",
    "\n",
    "\n",
    "        print(\"\\n\\nShowing normalised training data\")\n",
    "        print(train_normalized.head(5))\n",
    "        print(\"\\n\\nshowing train_x types of index 0\")\n",
    "        print(train_x[0])\n",
    "        print(\"\\n\\nshowing train_x types of index 1\")\n",
    "        print(train_x[1])\n",
    "        print(\"\\n number of nested array in train_x\")\n",
    "        print(len(train_x))\n",
    "        print(\"\\n\\ntrain_y types before processing\")\n",
    "        print(train_y.head(5))\n",
    "\n",
    "\n",
    "        # generate train labels (y)\n",
    "        seq_gen = []\n",
    "        start_index = 0\n",
    "        for i in range(train_engine_num):\n",
    "            end_index = start_index + train_rul.loc[i, 'max']\n",
    "            val = list(self.gen_labels(train_y.iloc[start_index:end_index, :], self.seq_len, train_y.columns))\n",
    "            seq_gen.extend(val)\n",
    "            start_index = end_index\n",
    "        train_y = list(seq_gen)\n",
    "\n",
    "        \n",
    "        print(\"\\n\\nshowing train_y types\")\n",
    "        print(np.shape(train_y))\n",
    "        print(str(train_y[0]))\n",
    "\n",
    "\n",
    "\n",
    "        seq_gen = []\n",
    "        start_index = 0\n",
    "        for i in range(test_engine_num):\n",
    "            end_index = start_index + test_rul.loc[i, 'max']\n",
    "            # diff@xuqing\n",
    "            # for test matrix, only 1 of n X 15 needed per engine, so the array input start from end index - sequence length\n",
    "            if end_index - start_index < self.seq_len:\n",
    "                # print('Sensor::test data ({:}) less than seq_len ({:})!'\n",
    "                #       .format(end_index - start_index, self.seq_len))\n",
    "\n",
    "                # simply pad the first data serveral times:\n",
    "                # print('Sensor::Use first data to pad!')\n",
    "                num_pad = self.seq_len - (end_index - start_index)\n",
    "                new_sg = test_normalized.iloc[start_index:end_index, :]\n",
    "                for idx in range(num_pad):\n",
    "                    new_sg = pd.concat([new_sg.head(1), new_sg], axis=0)\n",
    "\n",
    "                val = list(self.gen_sequence(new_sg, self.seq_len, test_normalized.columns))\n",
    "            else:\n",
    "                val = list(self.gen_sequence(test_normalized.iloc[end_index - self.seq_len:end_index, :], self.seq_len,\n",
    "                                             test_normalized.columns))\n",
    "            seq_gen.extend(val)\n",
    "            start_index = end_index\n",
    "        test_x = list(seq_gen)\n",
    "        # print(np.shape(test_y))\n",
    "\n",
    "        seq_gen = []\n",
    "        start_index = 0\n",
    "        for i in range(test_engine_num):\n",
    "            end_index = start_index + test_rul.loc[i, 'max']\n",
    "            # for test matrix, only 1 of n X 15 needed per engine, so the array input start from end index - sequence length\n",
    "            # print(end_index - start_index)\n",
    "            if end_index - start_index < self.seq_len:\n",
    "                # print('Setting::test data ({:}) less than seq_len ({:})!'\n",
    "                #       .format(end_index - start_index, self.seq_len))\n",
    "\n",
    "                # simply pad the first data serveral times:\n",
    "                # print('Setting::Use first data to pad!')\n",
    "                num_pad = self.seq_len - (end_index - start_index)\n",
    "                new_sg = test_setting.iloc[start_index:end_index, :]\n",
    "                for idx in range(num_pad):\n",
    "                    new_sg = pd.concat([new_sg.head(1), new_sg], axis=0)\n",
    "\n",
    "                val = list(self.gen_sequence(new_sg, self.seq_len, test_setting.columns))\n",
    "            else:\n",
    "                val = list(self.gen_sequence(test_setting.iloc[end_index - self.seq_len:end_index, :], self.seq_len,\n",
    "                                             test_setting.columns))\n",
    "\n",
    "            seq_gen.extend(val)\n",
    "            start_index = end_index\n",
    "        test_ops = list(seq_gen)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print('label starts')\n",
    "        seq_gen = []\n",
    "        start_index = 0\n",
    "        for i in range(test_engine_num):\n",
    "            end_index = start_index + test_rul.loc[i, 'max']\n",
    "            # print(end_index - start_index)\n",
    "            # print(end_index - self.seq_len)\n",
    "            if (end_index - self.seq_len) < 0:\n",
    "                val = list([self.gen_test_labels(test_y.iloc[0:end_index, :], self.seq_len, test_y.columns)])\n",
    "            else:\n",
    "                val = list([self.gen_test_labels(test_y.iloc[end_index - self.seq_len:end_index, :], self.seq_len,\n",
    "                                                 test_y.columns)])\n",
    "            seq_gen.extend(val)\n",
    "            start_index = end_index\n",
    "        test_y = list(seq_gen)\n",
    "\n",
    "        return train_x, train_y, test_x, test_y\n",
    "\n",
    "    def gen_sequence(self, id_df, seq_length, seq_cols):\n",
    "\n",
    "        # for one id I put all the rows in a single matrix\n",
    "        data_matrix = id_df[seq_cols].values.astype(np.float32)\n",
    "        num_elements = data_matrix.shape[0]\n",
    "        # Iterate over two lists in parallel.\n",
    "        # For example id1 (engine 1) have 192 rows and sequence_length is equal to 15\n",
    "        # so zip iterate over two following list of numbers (0,177),(14,191)\n",
    "        # 0 14 -> from row 0 to row 14\n",
    "        # 1 15 -> from row 1 to row 15\n",
    "        # 2 16 -> from row 2 to row 16\n",
    "        # ...\n",
    "        # 177 191 -> from row 177 to 191\n",
    "        for start, stop in zip(range(0, num_elements - seq_length + 1), range(seq_length, num_elements + 1)):\n",
    "            yield data_matrix[start:stop, :]\n",
    "\n",
    "    def gen_labels(self, id_df, seq_length, label):\n",
    "        # For example:\n",
    "        # [[1]\n",
    "        # [4]\n",
    "        # [1]\n",
    "        # [5]\n",
    "        # [9]\n",
    "        # ...\n",
    "        # [200]]\n",
    "        data_matrix = id_df[label].values\n",
    "        num_elements = data_matrix.shape[0]\n",
    "        label_matrix = []\n",
    "        for i in range(num_elements - (seq_length - 1)):\n",
    "            label_matrix.append(data_matrix[i + (seq_length - 1), :])\n",
    "\n",
    "        return label_matrix\n",
    "\n",
    "    # function to generate labels\n",
    "    def gen_test_labels(self, id_df, seq_length, label):\n",
    "        # For example:\n",
    "        # [[1]]\n",
    "        data_matrix = id_df[label].values\n",
    "        num_elements = data_matrix.shape[0]\n",
    "        # For the test labels, only 1 RUL is required per engine which is the last columns on each engine\n",
    "        return data_matrix[-1, :]\n",
    "    \n",
    "\n",
    "\n",
    "    def data_save(self):\n",
    "        if not os.path.exists('Processed_dataset'):\n",
    "            os.mkdir('Processed_dataset')\n",
    "        data_dir = os.path.join('Processed_dataset','CMAPSS')\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.mkdir(data_dir)\n",
    "        condition_data_dir = os.path.join(data_dir, f'{self.data_set}')\n",
    "        if not os.path.exists(condition_data_dir):\n",
    "            os.mkdir(condition_data_dir)\n",
    "\n",
    "        torch.save({'samples':self.train_x,'labels':self.train_y,'max_ruls':self.max_rul},f'{condition_data_dir}/train.pt')\n",
    "        torch.save({'samples':self.test_x,'labels':self.test_y,'max_ruls':self.max_rul},f'{condition_data_dir}/test.pt')\n",
    "\n",
    "\n",
    "ROOT_PATH = os.path.join(\"C:\\\\Monash\\\\Research_Intern\\\\RUL_brenchmark\\\\Frank-Wang-oss-GNN_RUL_Benchmarking\\\\Data_Process\", \"Datasets\")\n",
    "data = CMAPSS(ROOT_PATH, data_set='FD004', max_rul=125, seq_len=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
